% jam 2004-09-10

\section{Vector spaces}
\label{sec:vector-spaces}

My approach to vector spaces is largely based on
the texts I used as a college freshman for linear algebra and
multivariate calculus: Halmos \cite{Halmos:1958:Finite}
and Spivak \cite{Spivak:1965:CalculusOnManifolds}.

Some useful definitions/identities:

Let $\Space{V}$ be an $n$-dimensional real inner product space.
Let $\Vector{v}, \Vector{w} \in \Space{V}$.

\begin{itemize}
\item The inner (dot) product on $\Reals^n$:
\begin{equation}
\Vector{v} \bullet \Vector{w} \; \equiv \; \sum_{i=0}^{n-1} v_i w_i
\end{equation}

\item The euclidean ($l_2$) norm:
\begin{equation}
\| \Vector{v} \|^2 \; \equiv \; \Vector{v} \bullet \Vector{v}
\end{equation}

\item $\theta(\Vector{v},\Vector{w})$ is the angle between $\Vector{v}$ and $\Vector{w}$
and is defined by:
\begin{eqnarray}
\Vector{v} \bullet \Vector{w} \; = \; \| \Vector{v} \| \| \Vector{w} \| \cos(\theta(\Vector{v},\Vector{w}))
\\
\theta(\Vector{v},\Vector{w})
\; \equiv \;
\cos^{-1} \left(\frac{ \Vector{v} \bullet \Vector{w} }{\| \Vector{v} \| \| \Vector{w} \| } \right)
\nonumber
\end{eqnarray}

\item The tensor (outer) product:

Let $\Vector{v}, \Vector{u} \in \Space{V}, \Vector{w} \in \Space{W}.$
$\Vector{w} \otimes \Vector{v}$ is a rank 1 linear map
from $\Space{V}$ to $\Space{W}$, defined by:
\begin{equation}
(\Vector{w} \otimes \Vector{v})(\Vector{u}) \; \equiv \; \Vector{w} (\Vector{v} \bullet \Vector{u})
\end{equation}

Note: this is an abuse of the usual definition of tensor product $\otimes$.
This operation, which takes a pair of vectors and returns a linear map,
is more conventionally referred to as the 'outer product',
and written $\Vector{w} \Vector{v}^{\dagger}$.
However, because I am working in spaces other than $\Reals^n$
(eg. $\Space{L}space(\Space{V},\Space{W})$, the space of linear maps
between 2 vector spaces),
I want to avoid notations that suggest thinking in terms
of 'row' and 'column' vectors.

The following is a useful identity.
If $\Vector{t} \in \Space{T}$, $\Vector{u}, \Vector{v} \in \Space{V}$, 
and $\Vector{w} \in \Space{W}.$
then
\begin{equation}
\label{eq:tensor-dot}
(\Vector{t} \otimes \Vector{u}) (\Vector{v} \otimes \Vector{w})(\Vector{u}) = 
(\Vector{u} \bullet \Vector{v}) (\Vector{t} \otimes \Vector{w})
\end{equation}


\item Elementary orthogonal projection:
\begin{equation}
\Projection_{\Vector{w}} \Vector{v}
\; \equiv \;
\left( \frac{ \Vector{w} }{ \| \Vector{w} \| } \otimes \frac{ \Vector{w} }{ \| \Vector{w} \| } \right) \Vector{v}
\; = \;
\left( \frac{\Vector{w} }{\|\Vector{w}\|} \bullet \Vector{v} \right) \frac{\Vector{w}}{\|\Vector{w}\|}
\end{equation}

\item Orthogonal complement:
\begin{equation}
\perp_{\Vector{w}} \Vector{v}
\; \equiv \;
\Vector{v} \perp \Vector{w}
\; \equiv \;
\Vector{v} \; - \; \Projection_{\Vector{w}} \Vector{v}
\; = \;
\Vector{v} \; - \; \left( \frac{\Vector{w}}{\|\Vector{w}\|} \bullet \Vector{v} \right) \frac{\Vector{w}}{\|\Vector{w}\|}
\end{equation}

\end{itemize}

