\begin{plSection}{Maps between vector spaces}
\label{sec:maps}

In general, the functions discussed here map 
between real inner product spaces:
$\Vector{f}:\vspace \mapsto \Space{W}$, where $\vspace$ is the
{\it domain} and $\Space{W}$ is the {\it codomain}.
The real inner product spaces are almost all 
derived from some $\Reals^n$.

The {\it range} of $\Vector{f}$, $\range(\Vector{f})$, is the set 
$\Vector{f}(\vspace)$,
which may be a proper subset of its codomain $\Space{W}$.
The {\it kernel} of $\Vector{f}$, $\kernel(f)$, is the set
$\kernel(\Vector{f}) = \{ \Vector{v} \in \vspace : 
\Vector{f}(\Vector{v}) = \Vector{0} \}$.

I often use the words `map' and `function' interchangeably.
When I want to distinguish between real- and vector-valued functions,
I may use 'map' for vector-valued functions and
'function' for real-valued ones.

I use $Space{U}$, $\vspace$, $\Space{W}$ for generic vector spaces,
$\Vector{u}$, $\Vector{v}$, $\Vector{w}$, etc., for elements of vector spaces,
and
$\Vector{f}$, $\Vector{g}$, $\Vector{h}$ for vector-valued maps/functions.
I generally do not distinguish $\Reals$, the real numbers,
and $\Reals^1$, or any other 1-dimensional real vector space.
I sometimes use $f$, $g$, $h$ for extra clarity in the special
case of real-valued functions.

The domains of many interesting functions,
such as those that depend on vertex positions,
are direct sum of inner product spaces.
The {\it direct sum} $\vspace \oplus \Space{W}$ is 
the inner product space
consisting of the ordered pairs $\{ (\Vector{v},\Vector{w}) : 
\Vector{v} \in \vspace, \Vector{w} \in \Space{W} \}$
inheriting the inner product space operations in the obvious way:
$(\Vector{v}_0,\Vector{w}_0) \bullet (\Vector{v}_1,\Vector{w}_1) = 
(\Vector{v}_0 \bullet \Vector{v}_1) + (\Vector{w}_0 \bullet \Vector{w}_1).$
I will usually write an element of $\oplus^n \vspace$ as
$(\Vector{v}_0,\ldots,\Vector{v}_{n-1})$
and use
$\Vector{f}(\Vector{v}_0,\Vector{v}_1,\ldots,\Vector{v}_{n-1})$
for a function that depends on $n$ vectors.

%-----------------------------------------------------------------
\begin{plSection}{Linear maps}
\label{sec:linear-maps}

A map $\Vector{L}(\Vector{v}):\vspace \mapsto \Space{W}$
is {\it linear} iff
$\Vector{L}(a_0 \Vector{v}_0 + a_1 \Vector{v}_1) = a_0 \Vector{L}(\Vector{v}_0) + a_1 \Vector{L}(\Vector{v}_1)$.
I will often write $\Vector{L}\Vector{v} \equiv \Vector{L}(\Vector{v})$.

Its not hard to see that, for a linear map,
the range and kernel are linear subspaces of the codomain and
domain, respectively.
Thus any linear map between inner product spaces
divides its domain and codomain each into 2 orthogonal subspaces.
The domain is divided into $\vspace = \kernel(\Vector{L}) \oplus \kernel^{\perp}(\Vector{L})$,
and the codomain is divided into $\Space{W} = \range(\Vector{L}) \oplus \range^{\perp}(\Vector{L})$.

The most common representation for linear maps is the {\it matrix:}
Let $\Vector{L}(\Vector{v}):\vspace \mapsto \Space{W}$ be linear,
$\{ \Vector{e}_0^{\vspace} \ldots  \Vector{e}_{m-1}^{\vspace} \}$ an orthonormal basis for $\vspace$,
and
$\{ \Vector{e}_0^{\Space{W}} \ldots \Vector{e}_{n-1}^{\Space{W}} \}$ an orthonormal  basis for $\Space{W}$
Then $\Vector{L}$ can be expressed as
\begin{equation}
\Vector{L}
 =
\sum_{i=0}^{m-1} \sum_{j=0}^{n-1} L_{ij} ( \Vector{e}_i^{\Space{W}} \otimes \Vector{e}_j^{\vspace} )
\end{equation}
$(L_{ij})$ is the matrix representation of $\Vector{L}$ with respect to
the two bases\cite{Halmos:1958:Finite}.

It is important to note that there are many usful
representations for linear maps other than matrices \cite{McDonald:1989:OOPSLA}.
Sometimes other representations are used for convenience,
or to enforce some constraint like symmetry.
In some cases, a non-matrix representation must be used,
because a particular linear transformation
cannot be accurately represented by a matrix of floating point numbers.

Examples:

\begin{itemize}

\item Column-wise:
$\Vector{L} = 
\sum_{j=0}^{n-1} ( \Vector{c}_j^{\Vector{L}} 
\otimes \Vector{e}_j^{\vspace} )$

$\Vector{c}_j^{\Vector{L}} \in \Space{W}$ 
are the 'columns' of $\Vector{L}$.
$\linearspan\{ \Vector{c}_0^{\Vector{L}} 
\ldots \Vector{c}_{n-1}^{\Vector{L}} \} 
= \range(\Vector{L})$
(see \cref{sec:spans-and-projections}).

\item Row-wise:
$\Vector{L} = \sum_{i=0}^{m-1} ( \Vector{e}_i^{\Space{W}} \otimes  \Vector{r}_i^{\Vector{L}} )$

$\Vector{r}_i^{\Vector{L}} \in \vspace$ are the 'rows' of $\Vector{L}$.
$\linearspan\{ \Vector{r}_0^{\Vector{L}} \ldots \Vector{r}_{m-1}^{\Vector{L}} \} =  \kernel(\Vector{L})^{\perp}$
(see \autoref{sec:spans-and-projections}).

\item Householder:
$\Vector{h}_{\Vector{v}} = \Identity_{\vspace} - \frac{2}{\| \Vector{v} \|^2} (\Vector{v} \otimes \Vector{v})$

Householder maps are usually chosen to zero the elements of
a vector, or a row or column of a matrix, for a contiguous range of
indices, say, $[i_0,\ldots,i_n)$.

\end {itemize}

\end{plSection}%{Linear maps}
%-----------------------------------------------------------------
\begin{plSection}{Affine maps}
\label{sec:affine-maps}

A function $\Point{A}(\Vector{v}):\vspace \mapsto \Space{W}$
is {\it affine} if distributes over affine combinations:
$\Point{A}(\sum_{i=0}^{n-1} a_i \Vector{v}_i) = \sum_{i=0}^{n-1} a_i \Point{A}(\Vector{v}_i) $
for all $\{a_i\}$ such that $1 = \sum_{i=0}^{n-1} a_i$.
(Note that I am describing affine maps on vector (linear) spaces,
rather than the slightly more general notion of affine maps on affine spaces.)
Any linear map between vector spaces is automatically affine.
The other major class of affine maps on vector spaces are the translations.
A {\it translation,} 
$\Point{T}_{\Vector{t}}$, $\vspace \mapsto \vspace$,
simply adds a vector ($\Vector{t}$) to its argument:
$\Point{T}_{\Vector{t}} \Vector{v} = \Vector{v} + \Vector{t}$.
It's not hard to see that any affine map between two vector spaces
can be represented as the sum of a linear map and a translation.
A typical representation for a general affine map 
$\Point{A} : \vspace \mapsto \Space{W}$
is as a pair $(\Vector{L},\Vector{t})$ where 
$\Vector{L} : \vspace \mapsto \Space{W}$ is linear,
$\Vector{t} \in \Space{W}$, and $\Point{A}(\Vector{v}) = 
\Vector{L}(\Vector{v}) + \Vector{t}$.

\end{plSection}%{Affine maps}
%-----------------------------------------------------------------
\begin{plSection}{Spans and projections}
\label{sec:spans-and-projections}

Let $\vspace$ be an $n$-dimensional inner product space.

The {\it linear span} of a set of $m$ vectors in $\vspace$
is the set of linear combinations of those vectors:
\begin{equation}
\linearspan\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \} 
= \{\Vector{v} \in \vspace : \Vector{v} = \sum_{i=0}^{m-1} a_i \Vector{v}_i\}
\end{equation}
$\linearspan\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \}$ 
is a linear subspace of $\vspace$.

The {\it projection} $\Projection_{\Set{S}} \Vector{v}$ 
of a vector $\Vector{v} \in \vspace$
onto an arbitrary subset $\Set{S} \subset \vspace$
is the closest point in $\Set{S}$ to $\Vector{v}$.
Projection onto a linear subspace is a linear map and
can be computed by summing
elementary orthogonal projections onto an orthonormal basis 
for the subspace.

An orthonormal basis for 
$\linearspan\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \}$
(and $\linearspan\{ \Vector{v}_0 \ldots \Vector{v}_{m-1} \}^\perp$)
can be computed using the QR decomposition
of the map $\Vector{V} = \sum_{i=0}^{m-1} \Vector{v}_i \otimes \Vector{e}_i$,
(the $n \times m$ matrix whose columns are the $\Vector{v}_i$).
See Golub and Van Loan~\cite[sec.~5.2]{GolubVanLoan:1996}.

The {\it affine span} of a set of $m+1$ vectors in $\vspace$
is the set of affine combinations of those vectors:
\begin{equation}
\affinespan\{ \Point{p}_0 \ldots \Point{p}_{m} \} = \{\Vector{v} \in \vspace : \Vector{v} = \sum_{i=0}^{m} b_i \Point{p}_i;
1 = \sum_{i=0}^{m} b_i \}.
\end{equation}
$\affinespan\{ \Point{p}_0 \ldots \Point{p}_{m} \}$ 
is an affine subspace of $\vspace$.
$\Vector{b} = ( b_0 \ldots b_m )$ are {\it barycentric coordinates}
for $\Vector{v}$ with respect to 
$\{ \Point{p}_0 \ldots \Point{p}_{m} \}$.
The barycentric coordinates are unique if 
$\{ \Point{p}_0 \ldots \Point{p}_{m} \}$
are affinely independent.

Any affine subspace, $Space{A}$, of a vector space, $\vspace$ can be represented as
as a translation of a linear subspace of $\vspace$:
$Space{A} = Space{T}(Space{A}) + \Vector{t}$,
$Space{T}(Space{A})$ is the set of differences of elements of $Space{A}$,
a linear subspace of $\vspace$.
If $\Vector{t}$ is any element of $Space{A}$.
then projection onto $Space{A}$
can be computed as a translation of an orthogonal projection onto $Space{T}(Space{A})$:
$\Projection_{Space{A}} (\Point{p}) = \Vector{t} + \Projection_{Space{T}(Space{A})} (\Point{p} - \Vector{t})$.
Typically, we pick $\Vector{t}$ to be the smallest element of $Space{A}$.
Projection onto an affine space is clearly an affine map.

We can represent the affine span of a set of $m+1$ vectors
as a translation of a linear span:
\begin{equation}
\affinespan\{ \Point{p}_0 \ldots \Point{p}_{m} \} = 
\Point{p}_m + \linearspan\{\Vector{v}_0 \ldots \Vector{v}_{m-1}\}
\end{equation}
where $\Vector{v}_i = \Point{p}_i - \Point{p}_m$,
which allows us to compute the projection onto
$\affinespan\{ \Point{p}_0 \ldots \Point{p}_{m} \}$
again using the QR decomposition
of $\Vector{V} = \sum_{i=0}^{m-1} \Vector{v}_i \otimes \Vector{e}_i$.

\end{plSection}%{Spans and projections}
%-----------------------------------------------------------------
\begin{plSection}{Inverses and pseudo-inverses}
\label{sec:Inverses-and-pseudo-inverses}

A convenient definition for the {\it true inverse}
of a map $\Vector{f}(\Vector{v}):\vspace \mapsto \Space{W}$ is
$\Vector{f}^{-1}(\Vector{w}) = 
\{ \Vector{v} : \Vector{f}(\Vector{v}) = \Vector{w} \}$.
The usual definition of inverse treats $\Vector{f}^{-1}$
as a map from $\Space{W} \mapsto \vspace$,
which is undefined where the value of the true
inverse is not a set containing a single point.

For maps between inner product spaces,
the {\it pseudo-inverse}, $\Vector{f}^{-}$, 
is a map $\Space{W} \mapsto \vspace$
defined everywhere on $\Space{W}$.
Let $\hat{\Vector{w}}$ be an element of $\Space{W}$ closest to 
$\Vector{w}$
such that $\Vector{f}^{-1}(\Vector{w})$ is not empty.
Let $\hat{\Vector{v}}$ be a minimum norm element of 
$\Vector{f}^{-1}(\hat{\Vector{w}})$.
Then $\Vector{f}^{-}(\Vector{w}) = \hat{\Vector{v}}$.

If $\Vector{L}$ is linear, then it's not hard to see that
$\hat{\Vector{w}} = \pi_{\range(\Vector{L})} \Vector{w}$, 
the projection of $\Vector{w}$
on the range of $\Vector{L}$
and
$\hat{\Vector{v}}$ is the unique element of 
$\kernel^{\perp}(\Vector{L})$
such that $\Vector{L}(\hat{\Vector{v}}) = \hat{\Vector{w}}$.

The pseudo-inverse of a linear map can be characterized
by the four Moore-Penrose conditions
(see Golub and Van Loan\cite[sec.~5.5.4]{GolubVanLoan:1996}):
\begin{enumerate}
\item $\Vector{L} \Vector{L}^{-} \Vector{L} = \Vector{L}$
\item $\Vector{L}^{-} \Vector{L} \Vector{L}^{-} = \Vector{L}^{-}$
\item $\left( \Vector{L} \Vector{L}^{-} \right)^{\dagger} 
= \Vector{L} \Vector{L}^{-}$
\item $\left( \Vector{L}^{-} \Vector{L} \right)^{\dagger}
 = \Vector{L}^{-} \Vector{L}$
\end{enumerate}

When the 'columns' of $\Vector{L}$, $\Vector{r}_j^{\Vector{L}}$
($\Vector{L} = \sum_{j=0}^{n-1} 
( \Vector{L}_j^{\Space{W}} \otimes \Vector{e}_j^{\vspace} )$)
are linearly independent,
then a useful identity is:
\begin{equation}
\label{eq:full-rank-pseudo-inverse}
\Vector{L}^{-} = 
\left( \Vector{L}^{\dagger} \Vector{L} \right)^{-1} 
\Vector{L}^{\dagger}
\end{equation}

The pseudoinverse can be computed
using standard matrix decompositions such as
the QR and SVD \cite{GolubVanLoan:1996}.
The pseudoinverse is an example of a linear transformation
which should {\em not} be represented by a matrix
\cite{McDonald:1989:OOPSLA}.

If $\Point{A}$ is affine,
let $\Point{A} = \Vector{L} + \Vector{t}$,
where $\Vector{L}$ is linear,
and $\Vector{t}$ is an element of $\range(\Point{A})$.
Then $\Point{A}^{-}(\Vector{w}) = 
\Vector{L}^{-}( \Vector{w} - \Vector{t} )$.

\end{plSection}%{Inverses and pseudo-inverses}
%-----------------------------------------------------------------
\end{plSection}%{Maps between vector spaces}
%-----------------------------------------------------------------


